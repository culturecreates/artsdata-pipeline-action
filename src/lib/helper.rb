require_relative '../page_fetcher_service/page_fetcher'
require_relative '../page_fetcher_service/static_page_fetcher'
require_relative '../page_fetcher_service/headless_page_fetcher'
require_relative '../graph_fetcher_service/graph_fetcher'
require_relative '../graph_fetcher_service/linkeddata_graph_fetcher'
require_relative '../browser_service/browser'
require_relative '../browser_service/chrome_browser'
require_relative '../sparql_service/sparql'
require_relative '../file_saver_service/file_saver'
require_relative '../file_saver_service/github_saver'
require_relative '../notification_service/notification'
require_relative '../notification_service/webhook_notification'
require_relative '../databus_service/databus'
require_relative '../spider_crawler_service/spider_crawler'
require_relative '../url_fetcher_service/url_fetcher'
require_relative '../robots_txt_parser_service/robots_txt_parser'

require 'securerandom'

module Helper
  def self.check_mode_requirements(mode, config)
    required_map = {
      "fetch"      => %w[page_url],
      "push"       => %w[artifact publisher download_url],
      "fetch-push" => %w[page_url artifact publisher]
    }

    keys_to_check = required_map.select { |k, _| mode == k }.values.flatten.uniq
    missing = keys_to_check.select { |key| config[key].nil? }

    unless missing.empty?
      puts "Missing required parameter(s) in config file: #{missing.join(', ')}. Exiting..."
      exit(1)
    end
  end

  def self.get_headers(custom_user_agent)
    linkeddata_version = Gem::Specification.find_by_name('linkeddata').version.to_s
    { "User-Agent" => custom_user_agent == nil ? "artsdata-crawler/#{linkeddata_version}" : custom_user_agent }
  end

  def self.format_urls(page_url)
    page_url = page_url.split(',')
    base_url = page_url.first.split('/')[0..2].join('/')
    [page_url, base_url]
  end

  def self.get_page_fetcher(is_headless:, headers:)
    if is_headless
      PageFetcherService::HeadlessPageFetcher.new(headers: headers, browser: BrowserService::ChromeBrowser.new)
    else
      PageFetcherService::StaticPageFetcher.new(headers: headers)
    end
  end

  def self.get_url_fetcher(page_url:, base_url:, entity_identifier:, is_paginated:, offset:, page_fetcher:, robots_txt_content:)
    UrlFetcherService::UrlFetcher.new(
      page_url: page_url,
      base_url: base_url,
      entity_identifier: entity_identifier,
      is_paginated: is_paginated,
      offset: offset,
      page_fetcher: page_fetcher,
      robots_txt_content: robots_txt_content
    )
  end

  def self.get_graph_fetcher(headers:, page_fetcher:, sparql_path:, html_extract_config:)
    GraphFetcherService::LinkedDataGraphFetcher.new(
      headers: headers,
      page_fetcher: page_fetcher,
      sparql: SparqlService::Sparql.new(sparql_path),
      html_extract_config: html_extract_config
    )
  end

  def self.get_github_saver(repository:, file_name:, token:)
    FileSaverService::GitHubSaverService.new(
      repository: repository,
      path: file_name,
      message: 'Add data generated by the script',
      access_token: token,
      author_name: 'GitHub Actions',
      author_email: 'actions@github.com'
    )
  end

  def self.get_databus_service(artifact:, publisher:, repository:, databus_url:)
    DatabusService::Databus.new(
      artifact: artifact,
      publisher: publisher,
      repository: repository,
      databus_url: databus_url
    )
  end

  def self.get_spider_crawler(url:, page_fetcher:, sparql_path:, robots_txt_content:)
    SpiderCrawlerService::SpiderCrawler.new(
      url: url,
      page_fetcher: page_fetcher,
      sparql: SparqlService::Sparql.new(sparql_path),
      robots_txt_content: robots_txt_content
    )
  end

  def self.fetch_types(graph:)
    graph.query([nil, RDF.type, nil]).map(&:object).uniq
  end

  def self.get_robots_txt_content(base_url: , page_fetcher: )
    robots_txt_url = URI.join(base_url, '/robots.txt').to_s
    puts "Fetching robots.txt from #{robots_txt_url}..."
    robots_txt_data, _ = page_fetcher.fetcher_with_retry(page_url: robots_txt_url)
    RobotsTxtParser.parse(robots_txt_data)
  end

  def self.get_page_type(content_type)
    if content_type&.include?('xml')
      :xml
    elsif content_type&.include?('html')
      :html
    else
      :unknown
    end
  end

  def self.generate_metadata_file_content(metadata_content)
    uuid_org   = SecureRandom.uuid
    uuid_crawl = SecureRandom.uuid

    # data from the caller workflow 
    datafeed_uri = metadata_content['datafeed_uri']
    datafeed_url = metadata_content['datafeed_url']
    datafeed_title = metadata_content['datafeed_title']
    datafeed_name = metadata_content['datafeed_name']
    crawl_name = metadata_content['crawl_name']
    crawl_description = metadata_content['crawl_description']
    same_as = metadata_content['same_as']

    # data to be fetched during the crawl
    databus_id = metadata_content['databus_id']
    url_count = metadata_content['url_count']
    start_time = metadata_content['start_time']
    end_time = metadata_content['end_time']

    crawl_uri = "urn:crawl:#{uuid_crawl}"
    org_uri = "urn:organization:#{uuid_org}"

    jsonld = {
      "@context" => {
        "schema" => "http://schema.org/",
        "prov"   => "http://www.w3.org/ns/prov#",
        "adr"    => "http://kg.artsdata.ca/resource/",
        "xsd"    => "http://www.w3.org/2001/XMLSchema#"
      },
      "@graph" => [
        {
          "@id"   => datafeed_uri,
          "@type" => "schema:DataFeed",
          "schema:name" => datafeed_title,
          "schema:dataFeedElement" => { "@id" => datafeed_url }
        },
        {
          "@id" => org_uri,
          "@type" => "schema:Organization",
          "schema:name" => datafeed_name,
          "schema:sameAs" => same_as,
          "schema:url" => { "@id" => datafeed_url }
        },
        {
          "@id" => crawl_uri,
          "@type" => "prov:Activity",
          "schema:name" => crawl_name,
          "schema:description" => crawl_description,
          "prov:used" => { "@id" => datafeed_url },
          "prov:startedAtTime" => { "@value" => start_time, "@type" => "xsd:dateTime" },
          "prov:endedAtTime"   => { "@value" => end_time,   "@type" => "xsd:dateTime" },
          "prov:wasInformedBy" => { "@id" => "http://kg.artsdata.ca/resource/SpiderActivity"},
          "schema:additionalProperty" => {
            "@type" => "schema:PropertyValue",
            "schema:name" => "urlsCrawledCount",
            "schema:value" => url_count
          }
        },
          databus_id && {
          "@id" => databus_id,
          "@type" => "prov:Entity",
          "prov:wasGeneratedBy" => { "@id" => crawl_uri }
        },
        {
          "@id" => "adr:SpiderActivity",
          "@type" => "prov:Activity",
          "schema:name" => "Spider crawl",
          "schema:description" =>
            "A crawl using the Artsdata-pipeline-action without a CSS selector resulting in a site wide crawl."
        }
      ]
    }

    graph = RDF::Graph.new
    JSON::LD::API.toRdf(jsonld) do |statement|
      graph << statement
    end
    graph
  end

  def self.send_databus_notification(notification_instance, response)
    case response[:status]
    when :success
      notification_instance.send_notification(stage: 'databus_push', message: response[:message])
    when :error
      notification_instance.send_notification(stage: 'databus_push', message: "Error occurred: #{response[:message]}")
      exit(1)
    when :exception
      notification_instance.send_notification(stage: 'databus_push', message: "Exception occurred: #{response[:message]}")
      exit(1)
    else
      notification_instance.send_notification(stage: 'databus_push', message: "Unknown status: #{response[:status]}")
      exit(1)
    end
  end
end
