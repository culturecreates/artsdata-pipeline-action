require_relative '../page_fetcher_service/page_fetcher'
require_relative '../page_fetcher_service/static_page_fetcher'
require_relative '../page_fetcher_service/headless_page_fetcher'
require_relative '../graph_fetcher_service/graph_fetcher'
require_relative '../graph_fetcher_service/linkeddata_graph_fetcher'
require_relative '../browser_service/browser'
require_relative '../browser_service/chrome_browser'
require_relative '../sparql_service/sparql'
require_relative '../file_saver_service/file_saver'
require_relative '../file_saver_service/github_saver'
require_relative '../notification_service/notification'
require_relative '../notification_service/webhook_notification'
require_relative '../databus_service/databus'
require_relative '../spider_crawler_service/spider_crawler'
require_relative '../url_fetcher_service/url_fetcher'
require_relative '../robots_txt_parser_service/robots_txt_parser'

require 'securerandom'

module Helper
  def self.check_mode_requirements(mode, config)
    required_map = {
      "fetch"      => %w[page_url],
      "push"       => %w[artifact publisher download_url],
      "fetch-push" => %w[page_url artifact publisher]
    }

    keys_to_check = required_map.select { |k, _| mode == k }.values.flatten.uniq
    missing = keys_to_check.select { |key| config[key].nil? }

    unless missing.empty?
      puts "Missing required parameter(s) in config file: #{missing.join(', ')}. Exiting..."
      exit(1)
    end
  end

  def self.get_headers(custom_user_agent)
    linkeddata_version = Gem::Specification.find_by_name('linkeddata').version.to_s
    { "User-Agent" => custom_user_agent == nil ? "artsdata-crawler/#{linkeddata_version}" : custom_user_agent }
  end

  def self.format_urls(page_url)
    page_url = page_url.split(',')
    base_url = page_url.first.split('/')[0..2].join('/')
    [page_url, base_url]
  end

  def self.get_page_fetcher(is_headless:, headers:)
    if is_headless
      PageFetcherService::HeadlessPageFetcher.new(headers: headers, browser: BrowserService::ChromeBrowser.new)
    else
      PageFetcherService::StaticPageFetcher.new(headers: headers)
    end
  end

  def self.get_url_fetcher(page_url:, base_url:, entity_identifier:, is_paginated:, offset:, page_fetcher:, robots_txt_content:)
    UrlFetcherService::UrlFetcher.new(
      page_url: page_url,
      base_url: base_url,
      entity_identifier: entity_identifier,
      is_paginated: is_paginated,
      offset: offset,
      page_fetcher: page_fetcher,
      robots_txt_content: robots_txt_content
    )
  end

  def self.get_graph_fetcher(headers:, page_fetcher:, sparql_path:, html_extract_config:)
    GraphFetcherService::LinkedDataGraphFetcher.new(
      headers: headers,
      page_fetcher: page_fetcher,
      sparql: SparqlService::Sparql.new(sparql_path),
      html_extract_config: html_extract_config
    )
  end

  def self.get_github_saver(repository:, file_name:, token:)
    FileSaverService::GitHubSaverService.new(
      repository: repository,
      path: file_name,
      message: 'Add data generated by the script',
      access_token: token,
      author_name: 'GitHub Actions',
      author_email: 'actions@github.com'
    )
  end

  def self.get_databus_service(artifact:, publisher:, repository:, databus_url:)
    DatabusService::Databus.new(
      artifact: artifact,
      publisher: publisher,
      repository: repository,
      databus_url: databus_url
    )
  end

  def self.get_spider_crawler(url:, page_fetcher:, sparql_path:, robots_txt_content:)
    SpiderCrawlerService::SpiderCrawler.new(
      url: url,
      page_fetcher: page_fetcher,
      sparql: SparqlService::Sparql.new(sparql_path),
      robots_txt_content: robots_txt_content
    )
  end

  def self.fetch_types(graph:)
    graph.query([nil, RDF.type, nil]).map(&:object).uniq
  end

  def self.get_robots_txt_content(base_url: , page_fetcher: )
    robots_txt_url = URI.join(base_url, '/robots.txt').to_s
    puts "Fetching robots.txt from #{robots_txt_url}..."
    robots_txt_data, _ = page_fetcher.fetcher_with_retry(page_url: robots_txt_url)
    RobotsTxtParser.parse(robots_txt_data)
  end

  def self.get_page_type(content_type)
    if content_type&.include?('xml')
      :xml
    elsif content_type&.include?('html')
      :html
    else
      :unknown
    end
  end

  def self.generate_metadata_file_content(metadata_content)
    uuid_crawl = SecureRandom.uuid

    # data from the caller workflow 
    datafeed_uri = metadata_content['datafeed_uri']
    datafeed_url = metadata_content['datafeed_url']
    datafeed_title = metadata_content['datafeed_title']
    datafeed_name = metadata_content['datafeed_name']
    crawl_name = metadata_content['crawl_name']
    crawl_description = metadata_content['crawl_description']
    same_as = metadata_content['same_as']
    artsdara_uri = metadata_content['artsdata_uri']

    # data to be fetched during the crawl
    databus_id = metadata_content['databus_id']
    url_count = metadata_content['url_count']
    start_time = metadata_content['start_time']
    end_time = metadata_content['end_time']
    structured_score = metadata_content['structured_score'].to_s

    crawl_uri = "urn:crawl:#{uuid_crawl}"
    org_uri = "urn:organization:#{same_as.split('/').last}"

    jsonld = {
      "@context" => {
        "schema" => "http://schema.org/",
        "prov"   => "http://www.w3.org/ns/prov#",
        "adr"    => "http://kg.artsdata.ca/resource/",
        "xsd"    => "http://www.w3.org/2001/XMLSchema#"
      },
      "@graph" => [
        {
          "@id"   => datafeed_uri,
          "@type" => "schema:DataFeed",
          "schema:name" => datafeed_title,
          "schema:dataFeedElement" => { "@id" => datafeed_url }
        },
        {
          "@id" => org_uri,
          "@type" => "schema:Organization",
          "schema:name" => datafeed_name,
          "schema:sameAs" => { "@id" => same_as },
          "schema:url" => { "@id" => datafeed_url, "@type" => "schema:Website" }
        },
        artsdara_uri && {
          "@id" => org_uri,
          "schema:sameAs" => { "@id" => artsdara_uri }
        },
        {
          "@id" => crawl_uri,
          "@type" => "prov:Activity",
          "schema:name" => crawl_name,
          "schema:description" => crawl_description,
          "prov:used" => { "@id" => datafeed_url },
          "prov:startedAtTime" => { "@value" => start_time, "@type" => "xsd:dateTime" },
          "prov:endedAtTime"   => { "@value" => end_time,   "@type" => "xsd:dateTime" },
          "prov:wasInformedBy" => { "@id" => "http://kg.artsdata.ca/resource/SpiderActivity"},
          "schema:additionalProperty" => [
            {
            "@type" => "schema:PropertyValue",
            "schema:name" => "urlsCrawledCount",
            "schema:value" => url_count
            },
            {
            "@type" => "schema:PropertyValue",
            "schema:name" => "structuredScore",
            "schema:value" => {
              "@value" => structured_score,
              "@type"  => "xsd:decimal"
            }
            }
          ],
        },
          databus_id && {
          "@id" => databus_id,
          "@type" => "prov:Entity",
          "prov:wasGeneratedBy" => { "@id" => crawl_uri }
        },
        {
          "@id" => "adr:SpiderActivity",
          "@type" => "prov:Activity",
          "schema:name" => "Spider crawl",
          "schema:description" =>
            "A crawl using the Artsdata-pipeline-action without a CSS selector resulting in a site wide crawl."
        }
      ]
    }

    graph = RDF::Graph.new
    JSON::LD::API.toRdf(jsonld) do |statement|
      graph << statement
    end
    graph
  end

  def self.send_databus_notification(notification_instance, response)
    case response[:status]
    when :success
      notification_instance.send_notification(stage: 'databus_push', message: response[:message])
    when :error
      notification_instance.send_notification(stage: 'databus_push', message: "Error occurred: #{response[:message]}")
      exit(1)
    when :exception
      notification_instance.send_notification(stage: 'databus_push', message: "Exception occurred: #{response[:message]}")
      exit(1)
    else
      notification_instance.send_notification(stage: 'databus_push', message: "Unknown status: #{response[:status]}")
      exit(1)
    end
  end

  def self.get_nested_object(graph, subject, property_path)
    for property in property_path
      next_subject = nil
      graph.query([subject, property, nil]).each do |st|
        next_subject = st.object
        break
      end
      return nil if next_subject.nil?
      subject = next_subject
    end
    subject
  end

  def self.get_subjects_multitype(graph, types)
    subjects = Set.new
    types.each do |type|
      graph.query([nil, RDF.type, type]).each do |st|
        subjects.add(st.subject)
      end
    end
    subjects
  end


  def self.merge_graph(graph, new_graph)
    graph << new_graph

    new_events = self.get_subjects_multitype(new_graph, Config::SPIDER_CRAWLER[:event_types])

    new_events.each do |event|
      name = new_graph.query([event, RDF::Vocab::SCHEMA.name, nil]).first&.object
      start_date = new_graph.query([event, RDF::Vocab::SCHEMA.startDate, nil]).first&.object
      end_date = new_graph.query([event, RDF::Vocab::SCHEMA.endDate, nil]).first&.object
      location_name = self.get_nested_object(
        new_graph, 
        event, 
        [
          RDF::Vocab::SCHEMA.location, 
          RDF::Vocab::SCHEMA.name
        ]
      )
      postal_code = self.get_nested_object(
        new_graph, 
        event, 
        [
          RDF::Vocab::SCHEMA.location, 
          RDF::Vocab::SCHEMA.address, 
          RDF::Vocab::SCHEMA.postalCode
        ]
      )

      original_graph_events = self.get_subjects_multitype(graph, Config::SPIDER_CRAWLER[:event_types])

      duplicates = original_graph_events.select do |existing_event|
        existing_name = graph.query([existing_event, RDF::Vocab::SCHEMA.name, nil]).first&.object
        existing_start_date = graph.query([existing_event, RDF::Vocab::SCHEMA.startDate, nil]).first&.object
        existing_end_date = graph.query([existing_event, RDF::Vocab::SCHEMA.endDate, nil]).first&.object
        existing_location_name = self.get_nested_object(
          graph, 
          existing_event, 
          [
            RDF::Vocab::SCHEMA.location, 
            RDF::Vocab::SCHEMA.name
          ]
        )
        existing_postal_code = self.get_nested_object(
          graph, 
          existing_event, 
          [
            RDF::Vocab::SCHEMA.location, 
            RDF::Vocab::SCHEMA.address, 
            RDF::Vocab::SCHEMA.postalCode
          ]
        )
        existing_name == name &&
        existing_start_date == start_date &&
        (existing_end_date.nil? || end_date.nil? || existing_end_date == end_date) &&
        (existing_location_name.nil? || location_name.nil? || existing_location_name == location_name) &&
        (existing_postal_code.nil? || postal_code.nil? || existing_postal_code == postal_code)
      end

      next if duplicates.empty?


      chosen = ([event] + duplicates).max_by do |e|
        uri = e.to_s
        score = 0
        score += 2 unless e.node?
        score += 1 unless uri.include?("temporary")
        score
      end


      all_duplicates = duplicates + [event]
      to_remove = all_duplicates - [chosen]

      if to_remove.empty?
        next
      end

      puts "Removing duplicate events: #{to_remove.map(&:to_s).join(', ')}"
      puts "Keeping event: #{chosen.to_s}"

      to_remove.each do |bad_event|
        connected_entities = Set.new
        collect_connected_entities(graph, bad_event, connected_entities)
        to_delete = graph.statements.select { |st| connected_entities.include?(st.subject) }
        graph.delete(*to_delete)
      end

    end
    graph
  end

  def self.collect_connected_entities(graph, node, accumulator)
    return if accumulator.include?(node)
    accumulator.add(node)

    graph.query([node, nil, nil]).each do |st|
      obj = st.object
      if obj.node?
        collect_connected_entities(graph, obj, accumulator)
      end
    end
  end

end
